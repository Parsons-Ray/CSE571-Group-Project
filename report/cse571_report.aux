\relax 
\citation{sutton18}
\citation{sutton18}
\citation{sutton18}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{1}}
\newlabel{sec:background}{{II}{1}}
\@LN@col{2}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Sarsa\((\lambda )\) Algorithm with Dutch Tracing. Algorithm was provided by Sutton et al. \cite  {sutton18}\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{sarsa}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}SARSA(\(\lambda )\) Implementation}{1}}
\newlabel{sec:implementation}{{III}{1}}
\citation{ucbai}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Implementation of \texttt  {update} for Sarsa\((\lambda )\) agent. Since \texttt  {update} is called by the agent each time an aciton is selected, the function implements the inner loop of the algorithm detailed in Algorithm \G@refundefinedtrue {\unhbox \voidb@x \hbox {\normalfont  \bfseries  ??}}\GenericWarning  {               }{LaTeX Warning: Reference `sarsa' on page 2 undefined}.\relax }}{2}}
\newlabel{fig:sarsa-agent}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{2}}
\newlabel{sec:results}{{IV}{2}}
\@LN@col{2}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Summary of measured differences between Sarsa\((\lambda )\) and \(Q\)-learning agents. Time per 100 episodes (seconds) was taken from 20 trials using the \textit  {pacman.py} smallGrid world. Similarly, the agents were trained on an increasing number of episodes until they achieved a 50\% win rate in the \textit  {pacman.py} smallGrid world. Value convergence was found by exponentially increasing the number of training episodes until the \(Q\)-values and action policy became stationary. Value convergence was run using \textit  {gridworld.py} on the BookGrid world.\relax }}{2}}
\newlabel{summary}{{I}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \(Q\) values of the \(Q\)-learning and Sarsa\((\lambda )\) agents after 10 episodes of training. The Sarsa\((\lambda )\) agent is able to evaluate more of the state space than the \(Q\)-learning agent. Each agent was run for 10 episodes on the BookGrid world with a noise rate of 0.2, and \(\epsilon = 0.5\).\relax }}{2}}
\newlabel{qvalues}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Policies of the \(Q\)-learning and Sarsa\((\lambda )\) agents after 10 episodes on the LargeMazeGrid. Sarsa\((\lambda )\) agent's policy is almost on the verge of converging as opposed to that of the \(Q\)-learning agent.\relax }}{3}}
\newlabel{qvalues}{{3}{3}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \(Q\) values of the \(Q\)-learning and SARSA\((\lambda )\) agents after 100 episodes on the CustomGrid. Identical results are found for both the agents on this grid.\relax }}{3}}
\newlabel{qvalues}{{4}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{3}}
\@LN@col{2}
\newlabel{sec:conclusion}{{V}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Team Effectiveness}{3}}
\bibstyle{abbrv}
\bibdata{ref}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  default-pyg-prefix.pygstyle,
  891B96221E04E498BAF6B36C91298C9E1C1C31ADF369D02FD1DC6E4C5B7DDBA2.pygtex}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Win rate vs training episodes for \(Q\)-learning and Sarsa\((\lambda )\) agents. Each agent was trained using pacman smallGrid environment. Win rate was taken from 20 post training episodes. Each agent was trained with \(\epsilon = 0.05, \gamma = 0.8, \alpha = 0.2\). The Sarsa\((\lambda )\) agent has a default \(\lambda = 0.8\).\relax }}{4}}
\newlabel{winrate}{{5}{4}}
\@LN@col{1}
\newlabel{sec:team}{{VI}{4}}
\@LN@col{2}
